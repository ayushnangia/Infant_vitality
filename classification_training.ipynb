{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, GRU, Conv1D, MaxPooling1D, Flatten, Dense, GlobalAveragePooling1D, BatchNormalization, Activation, Add, Input\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_classes = ['N', 'L', 'R', 'e', 'j']\n",
    "abnormal_classes = ['A', 'a', 'J', 'S']\n",
    "\n",
    "beat_length = 50\n",
    "\n",
    "mapping = {'N': 0, 'L': 0, 'R': 0, 'e': 0, 'j': 0,\n",
    "        'A': 1, 'a': 1, 'J': 1, 'S': 1}\n",
    "\n",
    "samples = []\n",
    "sample_labels = []\n",
    "\n",
    "for j in range(100,235):\n",
    "    path = \"../dataset/MIT-BIH/{}\".format(j)\n",
    "    try:\n",
    "        signals, fields = wfdb.rdsamp(path)\n",
    "\n",
    "        annotation = wfdb.rdann(path, 'atr')\n",
    "\n",
    "        signal = signals[:, 0]  \n",
    "        labels = annotation.symbol\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] in normal_classes or labels[i] in abnormal_classes:\n",
    "                \n",
    "                beat_start = annotation.sample[i]\n",
    "                beat_end = annotation.sample[i+1] if i+1 < len(annotation.sample) else len(signal)\n",
    "                beat = signal[beat_start:beat_end]\n",
    "\n",
    "                \n",
    "                if len(beat) < beat_length:\n",
    "                    padded_beat = np.pad(beat, (0, beat_length - len(beat)), mode='constant')\n",
    "                    samples.append(padded_beat)\n",
    "                else:\n",
    "                    truncated_beat = beat[:beat_length]\n",
    "                    samples.append(truncated_beat)\n",
    "                sample_labels.append(mapping[labels[i]])\n",
    "\n",
    "        # for i in range(len(labels)):\n",
    "        #     if labels[i] in normal_classes or labels[i] in abnormal_classes:\n",
    "            \n",
    "        #         beat_start = annotation.sample[i]-int(beat_length/2)\n",
    "        #         beat_end = annotation.sample[i]+int(beat_length/2)\n",
    "        #         beat = signal[beat_start:beat_end]\n",
    "        #         samples.append(beat)\n",
    "        #         sample_labels.append(mapping[labels[i]])    \n",
    "        \n",
    "    \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "\n",
    "X = np.array(samples)\n",
    "y = np.array(sample_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.84 ,  0.765,  0.52 ,  0.17 , -0.165, -0.365, -0.435, -0.425,\n",
       "       -0.37 , -0.33 , -0.325, -0.335, -0.345, -0.33 , -0.325, -0.315,\n",
       "       -0.31 , -0.32 , -0.335, -0.34 , -0.325, -0.345, -0.335, -0.33 ,\n",
       "       -0.335, -0.33 , -0.325, -0.33 , -0.33 , -0.345, -0.355, -0.335,\n",
       "       -0.325, -0.305, -0.32 , -0.32 , -0.33 , -0.34 , -0.335, -0.34 ,\n",
       "       -0.345, -0.355, -0.355, -0.34 , -0.33 , -0.33 , -0.33 , -0.34 ,\n",
       "       -0.35 , -0.325])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93412"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.90      0.53      0.67       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.94      0.77      0.83     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# pickle.dump(rf, open('../model/randomForest', \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2336/2336 [==============================] - 7s 3ms/step - loss: 0.1094 - accuracy: 0.9697 - val_loss: 0.0901 - val_accuracy: 0.9715\n",
      "Epoch 2/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0928 - accuracy: 0.9734 - val_loss: 0.0841 - val_accuracy: 0.9788\n",
      "Epoch 3/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0877 - accuracy: 0.9766 - val_loss: 0.0798 - val_accuracy: 0.9794\n",
      "Epoch 4/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0836 - accuracy: 0.9782 - val_loss: 0.0819 - val_accuracy: 0.9798\n",
      "Epoch 5/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0800 - accuracy: 0.9793 - val_loss: 0.0743 - val_accuracy: 0.9810\n",
      "Epoch 6/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0779 - accuracy: 0.9800 - val_loss: 0.0715 - val_accuracy: 0.9824\n",
      "Epoch 7/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0758 - accuracy: 0.9802 - val_loss: 0.0739 - val_accuracy: 0.9792\n",
      "Epoch 8/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0736 - accuracy: 0.9807 - val_loss: 0.0698 - val_accuracy: 0.9823\n",
      "Epoch 9/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0726 - accuracy: 0.9810 - val_loss: 0.0721 - val_accuracy: 0.9830\n",
      "Epoch 10/10\n",
      "2336/2336 [==============================] - 6s 3ms/step - loss: 0.0709 - accuracy: 0.9814 - val_loss: 0.0673 - val_accuracy: 0.9831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.82      0.53      0.64       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.90      0.76      0.82     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_cnn = np.expand_dims(X_train, axis=-1).astype(np.float32)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=-1).astype(np.float32)\n",
    "# X_train_cnn = np.asarray(X_train_cnn).astype(np.float32)\n",
    "# X_test_cnn = np.asarray(X_test_cnn).astype(np.float32)\n",
    "\n",
    "model_cnn = Sequential()\n",
    "model_cnn.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(beat_length, 1)))\n",
    "model_cnn.add(MaxPooling1D(pool_size=2))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(units=64, activation='relu'))\n",
    "model_cnn.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_cnn.fit(X_train_cnn, y_train, batch_size=32, epochs=10, validation_data=(X_test_cnn, y_test))\n",
    "\n",
    "# _, accuracy = model_cnn.evaluate(X_test_cnn, y_test)\n",
    "# print(\"CNN Accuracy:\", accuracy)\n",
    "y_pred = model_cnn.predict(X_test_cnn)\n",
    "y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred] \n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2336/2336 [==============================] - 46s 19ms/step - loss: 0.1275 - accuracy: 0.9696 - val_loss: 0.1007 - val_accuracy: 0.9715\n",
      "Epoch 2/10\n",
      "2336/2336 [==============================] - 45s 19ms/step - loss: 0.1041 - accuracy: 0.9721 - val_loss: 0.0935 - val_accuracy: 0.9740\n",
      "Epoch 3/10\n",
      "2336/2336 [==============================] - 44s 19ms/step - loss: 0.0925 - accuracy: 0.9758 - val_loss: 0.0807 - val_accuracy: 0.9806\n",
      "Epoch 4/10\n",
      "2336/2336 [==============================] - 45s 19ms/step - loss: 0.0868 - accuracy: 0.9779 - val_loss: 0.0817 - val_accuracy: 0.9797\n",
      "Epoch 5/10\n",
      "2336/2336 [==============================] - 45s 19ms/step - loss: 0.0843 - accuracy: 0.9785 - val_loss: 0.0782 - val_accuracy: 0.9808\n",
      "Epoch 6/10\n",
      "2336/2336 [==============================] - 46s 20ms/step - loss: 0.0813 - accuracy: 0.9788 - val_loss: 0.0816 - val_accuracy: 0.9783\n",
      "Epoch 7/10\n",
      "2336/2336 [==============================] - 45s 19ms/step - loss: 0.0783 - accuracy: 0.9796 - val_loss: 0.0764 - val_accuracy: 0.9797\n",
      "Epoch 8/10\n",
      "2336/2336 [==============================] - 46s 19ms/step - loss: 0.0764 - accuracy: 0.9798 - val_loss: 0.0753 - val_accuracy: 0.9821\n",
      "Epoch 9/10\n",
      "2336/2336 [==============================] - 46s 20ms/step - loss: 0.0741 - accuracy: 0.9805 - val_loss: 0.0746 - val_accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "2336/2336 [==============================] - 45s 19ms/step - loss: 0.0712 - accuracy: 0.9810 - val_loss: 0.0645 - val_accuracy: 0.9833\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.80      0.55      0.65       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.90      0.77      0.82     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_gru = np.expand_dims(X_train, axis=-1)\n",
    "X_test_gru = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(units=64, input_shape=X_train_gru[0].shape))\n",
    "model_gru.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_gru.fit(X_train_gru, y_train, batch_size=32, epochs=10, validation_data=(X_test_gru, y_test))\n",
    "\n",
    "# _, accuracy = model_gru.evaluate(X_test_gru, y_test)\n",
    "# print(\"GRU Accuracy:\", accuracy)\n",
    "y_pred = model_gru.predict(X_test_gru)\n",
    "y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred] \n",
    "print(classification_report(y_test, y_pred_final))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     18150\n",
      "           1       0.84      0.42      0.56       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.91      0.71      0.78     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_svm, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test_svm)\n",
    "\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.85      0.55      0.67       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.92      0.77      0.83     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_xgb = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_xgb = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "xgboost = xgb.XGBClassifier()\n",
    "xgboost.fit(X_train_xgb, y_train)\n",
    "\n",
    "y_pred_xgb = xgboost.predict(X_test_xgb)\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.85      0.53      0.65       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.92      0.77      0.82     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_lgb = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_lgb = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "lgbm = lgb.LGBMClassifier()\n",
    "lgbm.fit(X_train_lgb, y_train)\n",
    "\n",
    "y_pred_lgb = lgbm.predict(X_test_lgb)\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2336/2336 [==============================] - 56s 23ms/step - loss: 0.0878 - accuracy: 0.9777 - val_loss: 0.0693 - val_accuracy: 0.9837\n",
      "Epoch 2/10\n",
      "2336/2336 [==============================] - 55s 23ms/step - loss: 0.0723 - accuracy: 0.9809 - val_loss: 0.0663 - val_accuracy: 0.9823\n",
      "Epoch 3/10\n",
      "2336/2336 [==============================] - 53s 23ms/step - loss: 0.0668 - accuracy: 0.9822 - val_loss: 0.0647 - val_accuracy: 0.9840\n",
      "Epoch 4/10\n",
      "2336/2336 [==============================] - 54s 23ms/step - loss: 0.0634 - accuracy: 0.9829 - val_loss: 0.0611 - val_accuracy: 0.9844\n",
      "Epoch 5/10\n",
      "2336/2336 [==============================] - 53s 23ms/step - loss: 0.0607 - accuracy: 0.9837 - val_loss: 0.0611 - val_accuracy: 0.9827\n",
      "Epoch 6/10\n",
      "2336/2336 [==============================] - 52s 22ms/step - loss: 0.0590 - accuracy: 0.9842 - val_loss: 0.0607 - val_accuracy: 0.9846\n",
      "Epoch 7/10\n",
      "2336/2336 [==============================] - 53s 23ms/step - loss: 0.0562 - accuracy: 0.9847 - val_loss: 0.0560 - val_accuracy: 0.9852\n",
      "Epoch 8/10\n",
      "2336/2336 [==============================] - 52s 22ms/step - loss: 0.0552 - accuracy: 0.9852 - val_loss: 0.0537 - val_accuracy: 0.9858\n",
      "Epoch 9/10\n",
      "2336/2336 [==============================] - 53s 23ms/step - loss: 0.0535 - accuracy: 0.9856 - val_loss: 0.0561 - val_accuracy: 0.9861\n",
      "Epoch 10/10\n",
      "2336/2336 [==============================] - 53s 23ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 0.0618 - val_accuracy: 0.9837\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [18683, 37366]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\InfantVitalityWatch\\InfantVitalityWatch\\main\\classification_training.ipynb Cell 18\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/InfantVitalityWatch/InfantVitalityWatch/main/classification_training.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m y_pred_ok\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/InfantVitalityWatch/InfantVitalityWatch/main/classification_training.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m y_pred_final \u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(i) \u001b[39mif\u001b[39;00m i\u001b[39m<\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m y_pred_ok] \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/InfantVitalityWatch/InfantVitalityWatch/main/classification_training.ipynb#X23sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred_final))\n",
      "File \u001b[1;32mc:\\Users\\Aditya Roy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:2110\u001b[0m, in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclassification_report\u001b[39m(\n\u001b[0;32m   1999\u001b[0m     y_true,\n\u001b[0;32m   2000\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2007\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2008\u001b[0m ):\n\u001b[0;32m   2009\u001b[0m     \u001b[39m\"\"\"Build a text report showing the main classification metrics.\u001b[39;00m\n\u001b[0;32m   2010\u001b[0m \n\u001b[0;32m   2011\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <classification_report>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2107\u001b[0m \u001b[39m    <BLANKLINE>\u001b[39;00m\n\u001b[0;32m   2108\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2110\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   2112\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2113\u001b[0m         labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Aditya Roy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Aditya Roy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18683, 37366]"
     ]
    }
   ],
   "source": [
    "def residual_block(x, filters, kernel_size, dilation_rate):\n",
    "    y = Conv1D(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='same')(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "\n",
    "    y = Conv1D(filters=filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    if x.shape[-1] != filters:\n",
    "        x = Conv1D(filters=filters, kernel_size=1, padding='same')(x)\n",
    "    y = Add()([x, y])\n",
    "    y = Activation('relu')(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv1D(filters=64, kernel_size=7, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block(x, filters=64, kernel_size=3, dilation_rate=1)\n",
    "    x = residual_block(x, filters=64, kernel_size=3, dilation_rate=2)\n",
    "    x = residual_block(x, filters=64, kernel_size=3, dilation_rate=4)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(units=num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "X_train_resnet = np.expand_dims(X_train, axis=-1)\n",
    "X_test_resnet = np.expand_dims(X_test, axis=-1)\n",
    "input_shape = X_train_resnet.shape[1:]\n",
    "\n",
    "model_resnet = build_resnet(input_shape, num_classes=2)\n",
    "\n",
    "\n",
    "model_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_resnet.fit(X_train_resnet, y_train, batch_size=32, epochs=10, validation_data=(X_test_resnet, y_test))\n",
    "\n",
    "# _, accuracy = model_resnet.evaluate(X_test_resnet, y_test)\n",
    "# print(\"ResNet Accuracy:\", accuracy)\n",
    "y_pred = model_resnet.predict(X_test_resnet)\n",
    "# y_pred = y_pred.reshape(1,-1)\n",
    "# y_pred_ok = y_pred[0]\n",
    "# y_pred_ok\n",
    "# y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred_ok] \n",
    "# print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18683"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_ok = y_pred\n",
    "# y_pred_ok\n",
    "# y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred_ok] \n",
    "# print(classification_report(y_test, y_pred_final))\n",
    "# y_pred = model_resnet.predict(X_test_resnet)\n",
    "# y_pred.shape[0]\n",
    "# len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.01      0.02     18150\n",
      "           1       0.01      0.30      0.02       533\n",
      "\n",
      "    accuracy                           0.02     18683\n",
      "   macro avg       0.14      0.16      0.02     18683\n",
      "weighted avg       0.27      0.02      0.02     18683\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     18150\n",
      "           1       0.72      0.70      0.71       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.86      0.84      0.85     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y_pred_0 = y_pred[:,0]\n",
    "# y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred_0] \n",
    "# print(classification_report(y_test, y_pred_final))\n",
    "y_pred_1 = y_pred[:,1]\n",
    "y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred_1] \n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2336/2336 [==============================] - 52s 22ms/step - loss: 0.1344 - accuracy: 0.9697 - val_loss: 0.1238 - val_accuracy: 0.9715\n",
      "Epoch 2/10\n",
      "2336/2336 [==============================] - 51s 22ms/step - loss: 0.1299 - accuracy: 0.9699 - val_loss: 0.1232 - val_accuracy: 0.9715\n",
      "Epoch 3/10\n",
      "2336/2336 [==============================] - 51s 22ms/step - loss: 0.1216 - accuracy: 0.9699 - val_loss: 0.1026 - val_accuracy: 0.9715\n",
      "Epoch 4/10\n",
      "2336/2336 [==============================] - 50s 22ms/step - loss: 0.1117 - accuracy: 0.9705 - val_loss: 0.0979 - val_accuracy: 0.9737\n",
      "Epoch 5/10\n",
      "2336/2336 [==============================] - 50s 22ms/step - loss: 0.0984 - accuracy: 0.9745 - val_loss: 0.0852 - val_accuracy: 0.9790\n",
      "Epoch 6/10\n",
      "2336/2336 [==============================] - 51s 22ms/step - loss: 0.0912 - accuracy: 0.9771 - val_loss: 0.0808 - val_accuracy: 0.9805\n",
      "Epoch 7/10\n",
      "2336/2336 [==============================] - 51s 22ms/step - loss: 0.0881 - accuracy: 0.9782 - val_loss: 0.0771 - val_accuracy: 0.9813\n",
      "Epoch 8/10\n",
      "2336/2336 [==============================] - 50s 21ms/step - loss: 0.0852 - accuracy: 0.9786 - val_loss: 0.0786 - val_accuracy: 0.9801\n",
      "Epoch 9/10\n",
      "2336/2336 [==============================] - 52s 22ms/step - loss: 0.0834 - accuracy: 0.9792 - val_loss: 0.0734 - val_accuracy: 0.9827\n",
      "Epoch 10/10\n",
      "2336/2336 [==============================] - 52s 22ms/step - loss: 0.0800 - accuracy: 0.9801 - val_loss: 0.0719 - val_accuracy: 0.9820\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     18150\n",
      "           1       0.77      0.53      0.63       533\n",
      "\n",
      "    accuracy                           0.98     18683\n",
      "   macro avg       0.88      0.76      0.81     18683\n",
      "weighted avg       0.98      0.98      0.98     18683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_lstm = np.expand_dims(X_train, axis=-1)\n",
    "X_test_lstm = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=64, input_shape=X_train_lstm[0].shape))\n",
    "model_lstm.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(X_train_lstm, y_train, batch_size=32, epochs=10, validation_data=(X_test_lstm, y_test))\n",
    "\n",
    "# _, accuracy = model_lstm.evaluate(X_test_lstm, y_test)\n",
    "# print(\"LSTM Accuracy:\", accuracy)\n",
    "y_pred = model_lstm.predict(X_test_lstm)\n",
    "y_pred_final = [int(i) if i<0.5 else 1 for i in y_pred] \n",
    "print(classification_report(y_test, y_pred_final))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
